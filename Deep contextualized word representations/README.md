# ELMo

## 论文提出或解决了什么问题

  * 多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。
    多义词对Word Embedding来说有什么负面影响？比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，
    是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，
    都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。
    所以word embedding无法区分多义词的不同语义。
    
    
## 论文中解决问题的关键元素是什么



![Image of ELMo](https://pic4.zhimg.com/v2-fe335ea9fdcd6e0e5ec4a9ac0e2290db_r.jpg)
  * ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作
    为新特征补充到下游任务中。上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词的上下文去正确预测。
    单词之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了     预测单词外的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM     叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子，句     子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更     多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，     还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。
    
    
    
![Image of ELMo](https://pic2.zhimg.com/80/v2-ef6513ff29e3234011221e4be2e97615_720w.jpg)
  * 上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问     题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个       Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为     X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给     下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。

## 论文中还有什么可以继续研究或提高

  * 提取特征的能力远远不如transformer
  * 双向LSTM只对于句子长度中等以下的有用，个人实验结果是200长度以下，否则极易gradient varnish。
  * 拼接方式双向融合hidden state并不能很好的表示融合后的信息
